const id = "introducing-mofa.md";
						const collection = "blog";
						const slug = "introducing-mofa";
						const body = "\nMoFA（**M**odular **F**ramework for **A**gent）是一个以组合的方式构建AI智能体的软件框架。使用MoFA，AI智能体可以通过模版方式构建，堆叠的方式组合，形成更强大的超级智能体（Super Agent）。\n\n## 🎯 设计理念\n\nMoFA 独特的设计理念是：\n\n- **平凡人做非凡事**：AI 不该是精英和巨头的专属领地。MoFA 让每个人都能驾驭和发展 AI，把不可能变成可能，让平凡人也能创造非凡。\n\n- **Composition AI**：受 Unix 哲学启发，MoFA 以\"组合\"作为核心原则。你可以像搭积木一样，构建智能体、连接智能体、集成工具，让 AI 变得简单、灵活、强大。\n\n- **Everything Agent**：与绝大多数软件不同，在 MoFA 的世界里，智能体（Agent）就是 AI 时代的应用（Application）。不仅是大语言模型，它可以是代码、脚本、API，甚至是 MoFA 本身。MoFA 不是一个框架，而是一个 Agent 生态。\n\n- **Data Flow**：大多数智能体框架依赖复杂的工作流（WorkFlow），而 MoFA 选择更直观、更强大的数据流（Data Flow）。这种方式让智能体能自由组合、拆解和重用。\n\n## 🏗️ 技术架构\n\n<img src=\"https://github.com/RelevantStudy/mofasearch/blob/main/hackathons/docs/images/image-20250310010710778.png\" alt=\"MoFA技术架构图\" style=\"zoom:67%;\" />\n\nMoFA 与 Dora-RS 形成分层架构，构成从底层通信到上层智能体的技术栈：\n\n```\n┌─────────────────────────────────────┐\n│           MoFA 层                   │  ← AI智能体开发框架\n│  智能体模板 + 组合逻辑 + 核心服务    │     (Python为主)\n├─────────────────────────────────────┤\n│           Dora 层                   │  ← 数据流引擎  \n│  实时通信 + 跨语言 + 运行时管理     │     (Rust核心)\n└─────────────────────────────────────┘\n```\n\n## 🚀 快速开始\n\n### 环境准备\n\n#### Python 环境\n```bash\n# 安装 UV 包管理器加速 mofa 安装\npip install uv\n```\n\n**注意事项**：\n- 本地python环境要纯净，不要多个python版本，否则容易导致Dora-rs运行环境和Mofa安装环境的冲突\n- 如果使用 Anaconda/Miniconda，务必将 Mofa 安装到 `Base` 环境下\n- 要求 python 环境 >= 3.10\n- 目前已在 WSL（Ubuntu 22.04）和 macOS 上测试，Windows 暂不支持\n\n#### Rust 环境\n```bash\n# 安装 Rust\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n\n# 安装 Dora 运行时\ncargo install dora-cli\n\n# 验证安装\ndora --version\n```\n\n### 安装 MoFA\n\n```bash\n# 克隆仓库\ngit clone https://github.com/moxin-org/mofa.git\ncd mofa/python\n\n# 安装依赖\nuv pip install -e . && pip install -e . \n```\n\n### Hello World 示例\n\n```bash\ncd mofa/python/examples/hello_world\n\n# 启动 Dora 服务\ndora up\n\n# 构建并运行数据流\ndora build hello_world_dataflow.yml\ndora start hello_world_dataflow.yml\n\n# 在另一个终端测试\nterminal-input\n> hello\n# 输出: hello\n```\n\n## 💡 5分钟创建第一个应用\n\n### 1. 创建 Agent 项目\n```bash\nmofa new-agent my-llm-agent\ncd my-llm-agent\n```\n\n### 2. 配置环境变量\n创建 `.env.secret` 文件：\n```plaintext\nLLM_API_KEY=your_api_key_here\nLLM_API_BASE=https://api.openai.com/v1\nLLM_MODEL=gpt-3.5-turbo\n```\n\n### 3. 实现 Agent 逻辑\n编辑 `my_llm_agent/main.py`：\n```python\nfrom mofa.agent_build.base.base_agent import MofaAgent, run_agent\nfrom openai import OpenAI\nimport os\nfrom dotenv import load_dotenv\n\n@run_agent\ndef run(agent: MofaAgent):\n    try:\n        load_dotenv('.env.secret')\n        \n        client = OpenAI(\n            api_key=os.getenv('LLM_API_KEY'),\n            base_url=os.getenv('LLM_API_BASE')\n        )\n        \n        user_input = agent.receive_parameter('query')\n        \n        response = client.chat.completions.create(\n            model=os.getenv('LLM_MODEL', 'gpt-3.5-turbo'),\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n                {\"role\": \"user\", \"content\": user_input}\n            ]\n        )\n        \n        agent.send_output(\n            agent_output_name='llm_result',\n            agent_result=response.choices[0].message.content\n        )\n        \n    except Exception as e:\n        agent.logger.error(f\"Error: {str(e)}\")\n        agent.send_output('llm_result', f\"Error: {str(e)}\")\n\ndef main():\n    agent = MofaAgent(agent_name='my-llm-agent')\n    run(agent=agent)\n\nif __name__ == \"__main__\":\n    main()\n```\n\n### 4. 创建数据流配置\n创建 `my_llm_dataflow.yml`：\n```yaml\nnodes:\n  - id: terminal-input\n    build: pip install -e ../../node-hub/terminal-input\n    path: dynamic\n    outputs: data\n    inputs:\n      agent_response: my-llm-agent/llm_result\n\n  - id: my-llm-agent\n    build: pip install -e . ../../agent-hub/my-llm-agent\n    path: my-llm-agent\n    outputs: llm_result\n    inputs:\n      query: terminal-input/data\n    env:\n      IS_DATAFLOW_END: true\n      WRITE_LOG: true\n```\n\n### 5. 运行和测试\n```bash\ndora up\ndora build my_llm_dataflow.yml\ndora start my_llm_dataflow.yml\n\n# 新开终端测试\nterminal-input\n> 你好，请介绍一下自己\n```\n\n## 🔧 构建自定义 Agent\n\n### 使用模板创建\n```bash\nmofa new-agent you_agent_name \n```\n\n### 核心代码示例\n```python\nfrom mofa.agent_build.base.base_agent import MofaAgent, run_agent\n\n@run_agent\ndef run(agent: MofaAgent):\n    try:\n        # 接收单个agent输入\n        task = agent.receive_parameter('task')\n        \n        # 接收多个agent输入\n        receive_datas = agent.receive_parameter(['example_one','example_two'])\n        # 结果类似于 {'example_one':'example_one_data','example_two':'example_two_data'}\n\n        # 处理逻辑 你可以把你的逻辑添加到这里\n        result = process_task(task)\n        \n        # 发送输出 确保你的输出是可以被序列化的对象(字符串等)\n        agent.send_output(\n            agent_output_name='agent_result',\n            agent_result=result\n        )\n        \n    except Exception as e:\n        agent.logger.error(f\"Error: {str(e)}\")\n        \n\ndef process_task(data: str) -> str:\n    \"\"\"示例处理函数\"\"\"\n    return f\"Processed: {data}\"\n\ndef main():\n    agent = MofaAgent(agent_name='my-new-agent')\n    run(agent=agent)\n\nif __name__ == \"__main__\":\n    main()\n```\n\n---\n\n立即开始您的智能体开发之旅！\n";
						const data = {title:"MoFA 开发框架：组合式 AI 智能体构建平台",description:"MoFA 是一个以组合的方式构建 AI 智能体的软件框架，让平凡人也能创造非凡的 AI 应用",date:new Date(1749427200000),author:"MoFA Team",tags:["架构","AI框架","Dora-RS","智能体","组合式AI"]};
						const _internal = {
							type: 'content',
							filePath: "/Users/liyao/Code/mofa-org.github.io/mofa-website/src/content/blog/introducing-mofa.md",
							rawData: "\ntitle: \"MoFA 开发框架：组合式 AI 智能体构建平台\"\ndescription: \"MoFA 是一个以组合的方式构建 AI 智能体的软件框架，让平凡人也能创造非凡的 AI 应用\"\ndate: 2025-06-09\nauthor: MoFA Team\ntags: [架构, AI框架, Dora-RS, 智能体, 组合式AI]",
						};

export { _internal, body, collection, data, id, slug };
